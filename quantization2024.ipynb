{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7mpZ4KTQTGTrXYNB7Y0zb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hideaki-kyutech/softcomp2024/blob/main/quantization2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 量子化: FP32 to BF16"
      ],
      "metadata": {
        "id": "TLfQ4aAWOGIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FP32でのFashionMNISTの分類モデルの作成"
      ],
      "metadata": {
        "id": "dnqc8CrdOPoW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9vb8-PFOXfl",
        "outputId": "163314d9-19da-44ec-bd43-5899923607ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 16.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 271kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.03MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 4.65MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "EPOCH 1:\n",
            "  batch 1000 loss: 1.7966186945587397\n",
            "  batch 2000 loss: 0.8062484353855253\n",
            "  batch 3000 loss: 0.6907540008034557\n",
            "  batch 4000 loss: 0.6144923003916629\n",
            "  batch 5000 loss: 0.5742344011918176\n",
            "  batch 6000 loss: 0.5155273887147196\n",
            "  batch 7000 loss: 0.5130124745445792\n",
            "  batch 8000 loss: 0.48083408589911414\n",
            "  batch 9000 loss: 0.4910981658545788\n",
            "  batch 10000 loss: 0.4774242734978907\n",
            "  batch 11000 loss: 0.43967159994679966\n",
            "  batch 12000 loss: 0.42548703986563485\n",
            "  batch 13000 loss: 0.4448238311446039\n",
            "  batch 14000 loss: 0.3879323586110259\n",
            "  batch 15000 loss: 0.40608691282640214\n",
            "LOSS train 0.40608691282640214 valid 0.39423666497434023\n",
            "EPOCH 2:\n",
            "  batch 1000 loss: 0.41024087067562504\n",
            "  batch 2000 loss: 0.35966621555329764\n",
            "  batch 3000 loss: 0.39620297122129705\n",
            "  batch 4000 loss: 0.3746347420369857\n",
            "  batch 5000 loss: 0.3604794119874132\n",
            "  batch 6000 loss: 0.3574897563600389\n",
            "  batch 7000 loss: 0.3472087559577194\n",
            "  batch 8000 loss: 0.35141011750503093\n",
            "  batch 9000 loss: 0.3535985640390354\n",
            "  batch 10000 loss: 0.36859683467276044\n",
            "  batch 11000 loss: 0.35039261814769995\n",
            "  batch 12000 loss: 0.3470646611215343\n",
            "  batch 13000 loss: 0.3439060791809461\n",
            "  batch 14000 loss: 0.33994201342537417\n",
            "  batch 15000 loss: 0.3377874554272421\n",
            "LOSS train 0.3377874554272421 valid 0.35375594775256175\n",
            "EPOCH 3:\n",
            "  batch 1000 loss: 0.3207051816036401\n",
            "  batch 2000 loss: 0.3319907327014807\n",
            "  batch 3000 loss: 0.3364746861883468\n",
            "  batch 4000 loss: 0.32018790082857596\n",
            "  batch 5000 loss: 0.32193159053120507\n",
            "  batch 6000 loss: 0.3008466511910083\n",
            "  batch 7000 loss: 0.3202969207371352\n",
            "  batch 8000 loss: 0.3007389299790957\n",
            "  batch 9000 loss: 0.3054065856525849\n",
            "  batch 10000 loss: 0.33054282369748034\n",
            "  batch 11000 loss: 0.3014575557046046\n",
            "  batch 12000 loss: 0.31781146242903197\n",
            "  batch 13000 loss: 0.3045223024241277\n",
            "  batch 14000 loss: 0.31672252872908574\n",
            "  batch 15000 loss: 0.3264910667114018\n",
            "LOSS train 0.3264910667114018 valid 0.32047372607858854\n",
            "EPOCH 4:\n",
            "  batch 1000 loss: 0.28520427323346664\n",
            "  batch 2000 loss: 0.29707922815220084\n",
            "  batch 3000 loss: 0.29358249226481714\n",
            "  batch 4000 loss: 0.30865246441932687\n",
            "  batch 5000 loss: 0.31019761289930464\n",
            "  batch 6000 loss: 0.3022021216669236\n",
            "  batch 7000 loss: 0.2839935462444846\n",
            "  batch 8000 loss: 0.27801506270002574\n",
            "  batch 9000 loss: 0.2923542873633305\n",
            "  batch 10000 loss: 0.28552716779258847\n",
            "  batch 11000 loss: 0.28488698129022944\n",
            "  batch 12000 loss: 0.28483317679948233\n",
            "  batch 13000 loss: 0.27534181112866646\n",
            "  batch 14000 loss: 0.29278749934739245\n",
            "  batch 15000 loss: 0.29428208212553\n",
            "LOSS train 0.29428208212553 valid 0.31371620074993845\n",
            "EPOCH 5:\n",
            "  batch 1000 loss: 0.2605217562566104\n",
            "  batch 2000 loss: 0.27457767661456456\n",
            "  batch 3000 loss: 0.276572913665128\n",
            "  batch 4000 loss: 0.26098791637414887\n",
            "  batch 5000 loss: 0.26314314861120747\n",
            "  batch 6000 loss: 0.27020930802111254\n",
            "  batch 7000 loss: 0.2572564961539465\n",
            "  batch 8000 loss: 0.25622784583766406\n",
            "  batch 9000 loss: 0.2908707995779841\n",
            "  batch 10000 loss: 0.2711549434942135\n",
            "  batch 11000 loss: 0.2671407714661873\n",
            "  batch 12000 loss: 0.28824290304357963\n",
            "  batch 13000 loss: 0.26691514619600637\n",
            "  batch 14000 loss: 0.27711260135624616\n",
            "  batch 15000 loss: 0.29303449160083256\n",
            "LOSS train 0.29303449160083256 valid 0.3057676647259381\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "# Define transformation to normalize the data: convert images to tensors and normalize them to a range of -1 to 1\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Create datasets for training & validation, download if not available\n",
        "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
        "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Create data loaders for our datasets\n",
        "# Shuffle the training data to ensure randomness; validation data does not need shuffling\n",
        "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
        "\n",
        "# Define class labels for easier interpretation of the results\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "# Define the model architecture for garment classification\n",
        "class GarmentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GarmentClassifier, self).__init__()\n",
        "        # Convolutional layer: input channel 1 (grayscale), output 6, kernel size 5x5\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        # Max pooling layer: kernel size 2x2, stride 2\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # Second convolutional layer: input 6, output 16, kernel size 5x5\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # Fully connected layer: input size 16*4*4, output size 120\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        # Fully connected layer: output size 84\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        # Final fully connected layer: output size 10 (for 10 classes)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply first convolutional layer, ReLU activation, and pooling\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        # Apply second convolutional layer, ReLU activation, and pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # Flatten the tensor for fully connected layers\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        # Apply fully connected layers with ReLU activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # Apply the final layer to get logits for 10 classes\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = GarmentClassifier()\n",
        "\n",
        "# Define loss function (cross-entropy for multi-class classification)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer (Stochastic Gradient Descent)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Function to train the model for one epoch\n",
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.0\n",
        "    last_loss = 0.0\n",
        "\n",
        "    # Enumerate over the training data\n",
        "    for i, data in enumerate(training_loader):\n",
        "        # Inputs and labels from the current batch\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero out gradients to prevent accumulation\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss and backpropagate\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update running loss and log every 1000 batches\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            last_loss = running_loss / 1000  # Average loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.0\n",
        "\n",
        "    return last_loss\n",
        "\n",
        "# Set up TensorBoard to visualize training metrics\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "# Number of epochs to train the model\n",
        "EPOCHS = 5\n",
        "\n",
        "# Initialize the best validation loss with a high value\n",
        "best_vloss = 1_000_000.0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Set the model to training mode to track gradients\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "    running_vloss = 0.0\n",
        "\n",
        "    # Set the model to evaluation mode (disable dropout and batchnorm updates)\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation for validation (saves memory and computation)\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(validation_loader):\n",
        "            vinputs, vlabels = vdata\n",
        "            # Forward pass for validation data\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = loss_fn(voutputs, vlabels)\n",
        "            running_vloss += vloss.item()\n",
        "\n",
        "    # Calculate average validation loss\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log training vs. validation loss to TensorBoard\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                       {'Training': avg_loss, 'Validation': avg_vloss},\n",
        "                       epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Save the model if the current validation loss is the best so far\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    # Increment the epoch counter\n",
        "    epoch_number += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_param_dtype(model):\n",
        "    # Iterate through all named parameters in the model\n",
        "    for name, param in model.named_parameters():\n",
        "        # Print the parameter name and its data type (dtype)\n",
        "        print(f\"{name} is loaded in {param.dtype}\")"
      ],
      "metadata": {
        "id": "zR1PDZInOcPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_param_dtype(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_r6QuDcQPd6",
        "outputId": "d90007be-b14c-4042-9842-7426d9c14892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight is loaded in torch.float32\n",
            "conv1.bias is loaded in torch.float32\n",
            "conv2.weight is loaded in torch.float32\n",
            "conv2.bias is loaded in torch.float32\n",
            "fc1.weight is loaded in torch.float32\n",
            "fc1.bias is loaded in torch.float32\n",
            "fc2.weight is loaded in torch.float32\n",
            "fc2.bias is loaded in torch.float32\n",
            "fc3.weight is loaded in torch.float32\n",
            "fc3.bias is loaded in torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習済みモデルのBF16への移植"
      ],
      "metadata": {
        "id": "KNURXwKMOetg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "x2_hc4gJQRgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_bf16 = deepcopy(model)"
      ],
      "metadata": {
        "id": "TRj9EOv2QTiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_bf16 = model_bf16.to(torch.bfloat16)"
      ],
      "metadata": {
        "id": "-m2VERO4QVkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_param_dtype(model_bf16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR8iHhttQXkC",
        "outputId": "216350ad-b7ab-421e-a705-84bc441895db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight is loaded in torch.bfloat16\n",
            "conv1.bias is loaded in torch.bfloat16\n",
            "conv2.weight is loaded in torch.bfloat16\n",
            "conv2.bias is loaded in torch.bfloat16\n",
            "fc1.weight is loaded in torch.bfloat16\n",
            "fc1.bias is loaded in torch.bfloat16\n",
            "fc2.weight is loaded in torch.bfloat16\n",
            "fc2.bias is loaded in torch.bfloat16\n",
            "fc3.weight is loaded in torch.bfloat16\n",
            "fc3.bias is loaded in torch.bfloat16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize metrics\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Disable gradient calculation for inference\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            # Forward pass: get predictions\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get class index with highest probability\n",
        "            # Update metrics\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate and return accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "_KIJXLDhQZPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test dataset\n",
        "test_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "QszumYUXQcGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_bf16(model, test_loader):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize metrics\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Disable gradient calculation for inference\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            # Convert input images to bfloat16 to match the model\n",
        "            images = images.to(torch.bfloat16)\n",
        "            # Forward pass: get predictions\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get class index with highest probability\n",
        "            # Update metrics\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate and return accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "NyCSk55KQeOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FP32モデルでの性能"
      ],
      "metadata": {
        "id": "YPer6c_ROpRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = evaluate_model(model, test_loader)\n",
        "print(f'Accuracy of the model on the test set 32 bits: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q5ZxzkuQhMI",
        "outputId": "67b86f1f-c54f-4c76-f726-12129d30a8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test set 32 bits: 88.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BF16モデルでの性能"
      ],
      "metadata": {
        "id": "4kdW9MxKOvrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = evaluate_model_bf16(model_bf16, test_loader)\n",
        "print(f'Accuracy of the model on the test set BF16: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_RMrvjXQjVS",
        "outputId": "e47e3961-b538-4eab-dcf7-7b6cdf726cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test set BF16: 88.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"garment_class_FP32.pth\")\n",
        "torch.save(model_bf16.state_dict(), \"garment_class_bf16.pth\")"
      ],
      "metadata": {
        "id": "-HYri0FDQnJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FP32モデルとBF16モデルのモデルサイズ確認(.pthファイルがモデルのファイル)"
      ],
      "metadata": {
        "id": "YTnq71eOOz6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls -al"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8zX9ESzQtBr",
        "outputId": "ed4f1a4e-c994-45e0-98ce-60a314bee751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1196\n",
            "drwxr-xr-x 1 root root   4096 Feb  5 22:37 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 1 root root   4096 Feb  5 22:26 \u001b[01;34m..\u001b[0m/\n",
            "drwxr-xr-x 4 root root   4096 Feb  4 14:22 \u001b[01;34m.config\u001b[0m/\n",
            "drwxr-xr-x 3 root root   4096 Feb  5 22:28 \u001b[01;34mdata\u001b[0m/\n",
            "-rw-r--r-- 1 root root  92498 Feb  5 22:37 garment_class_bf16.pth\n",
            "-rw-r--r-- 1 root root 181394 Feb  5 22:37 garment_class_FP32.pth\n",
            "-rw-r--r-- 1 root root 181528 Feb  5 22:29 model_20250205_222821_0\n",
            "-rw-r--r-- 1 root root 181528 Feb  5 22:30 model_20250205_222821_1\n",
            "-rw-r--r-- 1 root root 181528 Feb  5 22:32 model_20250205_222821_2\n",
            "-rw-r--r-- 1 root root 181528 Feb  5 22:33 model_20250205_222821_3\n",
            "-rw-r--r-- 1 root root 181528 Feb  5 22:34 model_20250205_222821_4\n",
            "drwxr-xr-x 3 root root   4096 Feb  5 22:28 \u001b[01;34mruns\u001b[0m/\n",
            "drwxr-xr-x 1 root root   4096 Feb  4 14:22 \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3bMDH8p6es55"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}